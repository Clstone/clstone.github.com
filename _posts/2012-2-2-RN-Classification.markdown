---
layout: post
title: "Reading notes: Review-分类，决策树,组合分类"
author: zhaocl
categories: memo
tags:   [Reading notes, Classification]
---

分类：

应用：包括欺诈检测、目标营销、性能预测、制造和医疗诊断。

无视其他基本概念：训练集、检验集、随机抽样等基本概念。

第一阶段，建立描述预先定义的数据类或概念集的分类器。也可以看做学习一个映射或函数 y=f(X).

由于提供了每个训练元组的类标号，这一阶段也称为监督学习（supervised learning）。它不同于无监督学习（unsupervised learning）(或聚类)

第二阶段，使用模型进行分类。

评估分类器的预测准确率，需要使用检验集。因为分类器趋向于过分拟合(overfit)训练集数据。

评估分类器可参考知识及资料。

statistic:  confusion matrices :sensitivity and positive predicted value, Kolmogorov-Smirnov statistic,  Wilcoxon-Mann-Whitney test , 
Graphics: ROC curve, the gains chart, the lift chart 
more details can be find in book: <<Predictive Modeling Using Logistic Regression>>Chapter 4 Measuring Classifier Performance


决策树归纳：

决策树（decision tree）是一种类似于流程图的树结构，其中，每个内部结点（非树叶结点,nodes）表示在一个属性上的测试，每个分枝代表该测试的一个输出，
而每个树叶结点（或终端结点，leaf）存放一个类标号。树的最顶层结点是根结点（root node）.

每个内部结点只分叉出两个其他结点为二叉树算法。多分叉为非二叉树。

决策树算法，ID3,C4.5,CART（Classification and Regression Tree）都采用贪心方法（即非回溯的），其中决策树以自顶向下递归的分支方式构造。

分裂属性(splitting criterion): 它指定分裂属性，并且也指出分裂点（splitting-point）或分裂子集（splitting subset）.ideally,分裂准则这样确定，
使得每个分枝上的输出分区都尽可能“纯”。（BTW,我喜欢纯的 :)）

属性选择度量：为描述给定训练元组的每个属性提供了秩评定，具有最好度量得分的属性被选为给定元组的分裂属性。

三种常用属性选择度量，信息增益、增益率、基尼指数（Gini指数）

1.信息增益

ID3使用信息增益作为属性选择度量。

对数据分区D中的元组分类所需要的期望信息由下式给出：
'\[
Info(D)=-\sum_{i=1}^m p_i log_2(p_i)
\]'

当按某个属性A划分之后，为了得到准确的分类，我们还需要的信息量
'\[
Info_A(D)=\sum_{j=1}^v\frac{D_j}{D} * Info(D_j)
\]'

信息增益定义为原来的信息需求与新的信息需求(对A划分后)之间的差。即
'\[
Gain(A)=Info(D)-Info_A(D)
\]'

换言之，Gain(A)告诉我们通过A上的划分我们得到了多少。它是知道A的值而导致的信息需求的期望减少。选择具有最高信息增益Gain(A)的
属性A作为结点N的分裂属性。这等价于在“能做到最佳分类”的属性A上划分，使得完成元组分类还需要的信息最小。

2.增益率
信息增益度量偏向具有许多输出的测试，它倾向于选择具有大量值的属性。C4.5使用增益率(gain ratio)的信息增益扩充，试图克服这种偏倚。
分裂信息值(split information):
'\[
SplitInfo_A(D)=-\sum_{i=1}^v\frac{D_j}{D} * log_2(\frac{D_j}{D}) (少个绝对值）
\]'

增益率
GrianRate(A) =  \frac{Gain(A)}{SplitInfo_A(D)}

选择具有最大增益率的属性作为分裂属性。NB: 随着划分信息趋向于0，该比率变得不稳定。


3.基尼系数

Gini Index 在CART（二叉决策树）使用。基尼系数度量数据区分或训练元组集D的不纯度，定义为

Gini(D)=1-

其中，p_i 是D中元组属于C_i类的概率，并用-----估计。

OOO:基尼指数考虑每个属性的二元划分。

当考虑二元划分裂时，计算每个结果分区的不纯度的加权和。如A的二元划分将D划分成D1和D2,则给定该划分，D的基尼指数为


属性A的二元划分导致的不纯度降低为


最大化不纯度降低（或等价地，具有最小基尼指数）的属性选为分裂属性。

4.其他流行属性选择度量：ＣＨＡＩＤ使用一种基于统计卡方检验的属性选择度量，　C-SEP,G-统计量，MDL。


树剪枝（pruning）

在决策树创建时，由于数据中的噪声和离群点，许多分枝反映的是训练数据中的异常。剪枝方法处理这种过分拟合数据问题。

两种常用的剪枝方法：先剪枝(Prepruning)和后剪枝(Postpruning)

Prepruning: 通过提前停止树的构建（如，通过决定在给定的结点不再分裂或划分训练元组的子集）而对树“剪枝”。一旦停止，结点就成为树叶。构造树时，使用统计显著性、信息增益、基尼指数
等度量与预定义阈值比较确定停止点。难点：选取适当的阈值，高阈值可能导致过分简化的树，而低阈值可能使得树的简化太少。

Postpruning:它由“完全生长”的树剪去子树。通过删除结点的分枝并用树叶替换它而剪掉给定结点上的子树。

CART使用postpruning的代价复杂度剪枝算法。该方法把树的复杂度看作树中树叶结点的个数和树的错误率的函数（其中，错误率是树误分类的元组所占的百分比），
使用的是一个标记类元组的剪枝集来评估代价复杂度，使用的是独立于建立未剪枝树的训练集和检验集的数据集。

C4.5使用的悲观剪枝方法利用的是检验集做评估。因为基于训练集评估准确率或错误率过于乐观。



决策树归纳的可视化挖掘

基于感知的分类（Perception-based Classification,PBC）是一种基于多维可视化技术的交互式方法，允许用户在构建决策树时加上关于数据的背景知识。

贝叶斯分类方法

朴素贝叶斯分类法假定一个属性值在给定类上的影响独立于其他属性的值。这一假定称为类条件独立性。做此假定是为了简化计算，并在此意义上称为“朴素的”，
朴素贝叶斯分类法预测X属于拥有最大后验假设概率的类。
该分类法与决策树和神经网络分类法的各种比较实验表明，在某些领域，贝叶斯分类法足以与它们相媲美。理论上讲，与其他所有分类算法相比，贝叶斯分类
法具有最小的错误率。然后，实践中并非总是如此。这是由于对其使用的假定（如类条件独立性）的不正确性，以及缺乏可用的概率数据造成的。


组合分类方法

装袋、提升和随机森林都是组合分类方法的例子。组合分类把k个学习得到的模型（或基分类器）M1,M2,...Mk 组合在一起，旨在创建一个改进的复合分类模型
M*。使用给定的数据集D创建k个训练集D1,D2,....,Dk,其中Di用于创建分类器Mi。给定一个待分类的新数据元组，每个基分类器通过返回类预测投票。组合分类器
基于基分类器的投票返回类预测。


More details in <数据挖掘 概念与技术 第三版>
